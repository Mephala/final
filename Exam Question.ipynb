{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Matrix Factorization\n",
    "\n",
    "Maximize the following :\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(W, H) = \\sum_{i=1}^I \\sum_{j=1}^J M(i,j) ( Y(i,j) \\log (\\sigma(W(i) H(j))) + (1 - Y(i,j)) \\log(1 - \\sigma(W(i) H(j))) )\n",
    "$$\n",
    "\n",
    "Observed $I\\times J$ binary matrix with possibly missing entries\n",
    "$Y(i,j) \\in \\{0,1\\}$\n",
    "\n",
    "Mask Matrix\n",
    "$M(i,j) = 1$ if $Y(i,j)$ is observed, $M(i,j) = 0$ if $Y(i,j)$ is not observed\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "$\\sigma(x)$ is the sigmoid function defined as\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{1}{1+e^{-x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "### Properties of the sigmoid function\n",
    "Note that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{e^x}{(1+e^{-x})e^x} = \\frac{e^x}{1+e^{x}} \\\\\n",
    "1 - \\sigma(x) & = & 1 - \\frac{e^x}{1+e^{x}} = \\frac{1+e^{x} - e^x}{1+e^{x}} = \\frac{1}{1+e^{x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma'(x) & = & \\frac{e^x(1+e^{x}) - e^{x} e^x}{(1+e^{x})^2} = \\frac{e^x}{1+e^{x}}\\frac{1}{1+e^{x}} = \\sigma(x) (1-\\sigma(x))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log \\sigma(x) & = & -\\log(1+e^{-x}) = x - \\log(1+e^{x}) \\\\\n",
    "\\log(1 - \\sigma(x)) & = &  -\\log({1+e^{x}})\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Evaluating the gradient \n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}(W,H)}{dW(i)} = \\sum_{j=1}^J (M(i,j) (Y(i,j) -\\sigma(W(i) H(j)))) H(j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathcal{L}(W,H)}{dH(j)} = \\sum_{i=1}^I  W(i) (M(i,j) (Y(i,j) -\\sigma(W(i) H(j))))\n",
    "$$\n",
    "\n",
    "\n",
    "Then use alternating gradient descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W and H used to generate problem matrix: W= [[-0.87062417]\n",
      " [-0.79065176]\n",
      " [ 0.77778248]\n",
      " [ 0.18022026]\n",
      " [ 0.88964628]] , H = [[ 0.98643023 -0.4016104   0.36099611 -0.69621904 -0.47657641 -0.35771474\n",
      "  -0.83280653 -0.03511777 -0.2451144  -0.9655413 ]]\n",
      "===============================\n",
      "W and H found after gradient solution: W= [[-0.64535762]\n",
      " [-0.74629652]\n",
      " [-0.23971459]\n",
      " [ 0.44929422]\n",
      " [-0.84119892]] , H = [[ 0.14874123 -1.18787148 -0.3098156  -0.11798829 -1.33497689 -0.88957889\n",
      "  -0.46182509 -0.62079179  0.1958358  -0.67291302]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# Generate a random logistic regression problem\n",
    "\n",
    "def sigmoid(t):\n",
    "    return np.exp(t)/(1+np.exp(t))\n",
    "\n",
    "\n",
    "\n",
    "I = 5\n",
    "J = 10\n",
    "\n",
    "# Random Mask \n",
    "M = np.random.rand(I,J)<0.8\n",
    "\n",
    "# Random Parameters\n",
    "W = np.random.randn(I,1)\n",
    "H = np.random.randn(1,J)\n",
    "\n",
    "Y = np.zeros((I,J))\n",
    "# Generate class labels\n",
    "pi = sigmoid(W*H)\n",
    "#pid = sigmoidd(W*H)\n",
    "\n",
    "for i in range(I):\n",
    "    for j in range(J):\n",
    "        if not M[i,j]:\n",
    "            Y[i,j] = np.nan\n",
    "        else:\n",
    "            Y[i,j] = 1 if pi[i,j] < np.random.rand() else 0\n",
    "            \n",
    "\n",
    "#Solution starts here\n",
    "\n",
    "\n",
    "print('W and H used to generate problem matrix: W=' , W, ', H =', H)\n",
    "\n",
    "\n",
    "\n",
    "def deriveH(i,j):\n",
    "    sum = 0\n",
    "    for it in range(I):\n",
    "        m = 1\n",
    "        if M[it][j] is None:\n",
    "            m = 0\n",
    "        tmp = W[it]*((m*Y[it][j]) - sigmoid(W[it]*H[0][j]))\n",
    "        if np.isnan(tmp):\n",
    "            tmp = 0\n",
    "        sum = sum + tmp\n",
    "    return sum\n",
    "        \n",
    "\n",
    "#def deriveW(i,j):\n",
    "#    if not M[i,j]:\n",
    "#        return 0\n",
    "#    else:\n",
    "#        (H[0][j]*(((Y[i,j]-1)*np.exp(W[i]*H[0][j]))+Y[i,j]))/(np.exp(H[0][j]*W[i]) + 1)\n",
    "\n",
    "def deriveW(i,j):\n",
    "    sum = 0\n",
    "    for jt in range(J):\n",
    "        m = 1\n",
    "        if M[i][jt] is None:\n",
    "            m = 0\n",
    "        tmp = ((m*Y[i][jt]) - (sigmoid(W[i]*H[0][jt])))*H[0][jt]\n",
    "        if np.isnan(tmp):\n",
    "            tmp = 0\n",
    "        sum = sum + tmp\n",
    "    return sum\n",
    "\n",
    "\n",
    "#for i in range(I):\n",
    "#    for j in range(J):\n",
    "#        print(deriveH(i,j))\n",
    "#        print(deriveW(i,j))\n",
    "\n",
    "\n",
    "print('===============================')\n",
    "\n",
    "#Assign new randoms to start gradient\n",
    "Wi = W\n",
    "Hi = H\n",
    "W = np.random.randn(I,1)\n",
    "H = np.random.randn(1,J)\n",
    "\n",
    "\n",
    "trials = 1\n",
    "epoch = 50\n",
    "eta = 0.0003366\n",
    "for t in range(trials):\n",
    "    for e in range(epoch):\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "                derW = deriveW(i,j)\n",
    "                if(derW is not None):\n",
    "                    #print('Wi = ' , Wi[i] , 'W = ' , W[i] ,'derivative = ', derW , ' value change:', (-(eta*derW)))\n",
    "                    W[i] = W[i] - (eta*derW)\n",
    "                derH = deriveH(i,j)\n",
    "                if(derH is not None):\n",
    "                    H[0][j] = H[0][j] - (eta*derH)\n",
    "                #derW = deriveW(i,j)\n",
    "                    #if(derW is not None):\n",
    "                        #print('Wi = ' , Wi[i] , 'W = ' , W[i] ,'derivative = ', derW , ' value change:', (-(eta*derW)))\n",
    "                        #W[i] = W[i] - (eta*derW)\n",
    "                    #derH = deriveH(i,j)\n",
    "                    #if(derH is not None):\n",
    "                        #H[0][j] = H[0][j] - (eta*derH)\n",
    "                #else:\n",
    "                    #derW = deriveW(i,j)\n",
    "                    #if(derW is not None):\n",
    "                        #print('Wi = ' , Wi[i] , 'W = ' , W[i] ,'derivative = ', derW , ' value change:', (-(eta*derW)))\n",
    "                        #W[i] = W[i] - (eta*derW)\n",
    "                    #derH = deriveH(i,j)\n",
    "                    #if(derH is not None):\n",
    "                        #H[0][j] = H[0][j] - (eta*derH)\n",
    "\n",
    "print('W and H found after gradient solution: W=' , W, ', H =', H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: \n",
    "Given $Y$ and $M$ only find a good $W$ and $H$ by maximizing the objective $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
